{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking with sktime\n",
    "\n",
    "The benchmarking modules allows you to easily orchestrate benchmarking experiments in which you want to compare the performance of one or more algorithms over one or more data sets. It also provides a number of statistical tests to check if observed performance differences are statistically significant.\n",
    "\n",
    "The benchmarking modules is based on mlaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required functions and classes\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sktime.benchmarking.data import UEADataset, make_datasets\n",
    "from sktime.benchmarking.evaluation import Evaluator\n",
    "from sktime.benchmarking.metrics import PairwiseMetric\n",
    "from sktime.benchmarking.orchestration import Orchestrator\n",
    "from sktime.benchmarking.results import HDDResults\n",
    "from sktime.benchmarking.strategies import TSCStrategy\n",
    "from sktime.benchmarking.tasks import TSCTask\n",
    "from sktime.classification.interval_based import (\n",
    "    RandomIntervalSpectralEnsemble,\n",
    "    TimeSeriesForestClassifier,\n",
    ")\n",
    "from sktime.series_as_features.model_selection import PresplitFilesCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up paths to data and results folder\n",
    "import sktime\n",
    "\n",
    "DATA_PATH = os.path.join(os.path.dirname(sktime.__file__), \"datasets/data\")\n",
    "RESULTS_PATH = \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pointers to datasets on hard drive\n",
    "Here we use the UEADataset which follows the UEA/UCR format and some of the time series classification datasets included in sktime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual pointers to dataset on the disk\n",
    "datasets = [\n",
    "    UEADataset(path=DATA_PATH, name=\"ArrowHead\"),\n",
    "    UEADataset(path=DATA_PATH, name=\"ItalyPowerDemand\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UEADataset(name=ArrowHead), UEADataset(name=ItalyPowerDemand)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each dataset, we also need to specify a learning task\n",
    "The learning task encapsulate all the information and instructions that define the problem we’re trying to solve. In our case, we’re trying to solve classification tasks and the key information we need is the name of the target variable in the data set that we’re trying to predict. Here all tasks are the same because the target variable has the same name in all data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [TSCTask(target=\"target\") for _ in range(len(datasets))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify learning strategies\n",
    "\n",
    "Having set up the data sets and corresponding learning tasks, we need to define the algorithms we want to evaluate and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify learning strategies\n",
    "strategies = [\n",
    "    TSCStrategy(TimeSeriesForestClassifier(n_estimators=10), name=\"tsf\"),\n",
    "    TSCStrategy(RandomIntervalSpectralEnsemble(n_estimators=10), name=\"rise\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a results object\n",
    "\n",
    "The results object encapsulates where and how benchmarking results are stored, here we choose to output them to the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/01D7B0AB8D0B4060/sktime/sktime/benchmarking/base.py:158: UserWarning: Existing results file found in given path: results. Results file will be updated\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Specify results object which manages the output of the benchmarking\n",
    "results = HDDResults(path=RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run benchmarking\n",
    "\n",
    "Finally, we pass all specifications to the orchestrator. The orchestrator will automatically train and evaluate all algorithms on all data sets and write out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run orchestrator\n",
    "orchestrator = Orchestrator(\n",
    "    datasets=datasets,\n",
    "    tasks=tasks,\n",
    "    strategies=strategies,\n",
    "    cv=PresplitFilesCV(),\n",
    "    results=results,\n",
    ")\n",
    "orchestrator.fit_predict(save_fitted_strategies=False, overwrite_predictions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and compare results\n",
    "\n",
    "Having run the orchestrator, we can evaluate and compare the prediction strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>accuracy_mean</th>\n",
       "      <th>accuracy_stderr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>0.778017</td>\n",
       "      <td>0.022043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tsf</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.020209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  strategy  accuracy_mean  accuracy_stderr\n",
       "0     rise       0.778017         0.022043\n",
       "1      tsf       0.836735         0.020209"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = Evaluator(results)\n",
    "metric = PairwiseMetric(func=accuracy_score, name=\"accuracy\") #noqa\n",
    "metrics_by_strategy = evaluator.evaluate(metric=metric)\n",
    "metrics_by_strategy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluator offers a number of additional methods for evaluating and comparing strategies, including statistical hypothesis tests and visualisation tools, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strategy</th>\n",
       "      <th>accuracy_mean_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rise</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tsf</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  strategy  accuracy_mean_rank\n",
       "0     rise                 2.0\n",
       "1      tsf                 1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.rank()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c690c4aacf52c5b35e8c405d3ffa01c0fb45d098d01a1254237bffad00a0de1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sktime-gluon-dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
